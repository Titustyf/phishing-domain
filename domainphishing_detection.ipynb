{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5SDTRZLnIbBfrDpUtxHVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Titustyf/phishing-domain/blob/main/domainphishing_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V_XmCRrg964B",
        "outputId": "85ad71b7-6703-42d7-ea5a-11c417c27abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (23.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Using cached protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.71.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Using cached protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: protobuf, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.9.1\n",
            "    Uninstalling keras-3.9.1:\n",
            "      Successfully uninstalled keras-3.9.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorstore 0.1.72 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 protobuf-4.25.6 tensorboard-2.15.2 tensorflow-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "e68ef9df9d95414489978ed7d16579b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvwmpn59_Col",
        "outputId": "6429f917-21ae-4af6-9dc7-9b7721f12425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tensorflow-text, tensorflow_decision_forests, tensorflowjs, tf_keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd_5-1AbNBFe",
        "outputId": "185c637c-85f8-4967-c2f9-a2df0f44c7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-30 17:44:15--  https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25067193 (24M) [text/plain]\n",
            "Saving to: ‘phishing_dataset.csv’\n",
            "\n",
            "phishing_dataset.cs 100%[===================>]  23.91M   117MB/s    in 0.2s    \n",
            "\n",
            "2025-03-30 17:44:16 (117 MB/s) - ‘phishing_dataset.csv’ saved [25067193/25067193]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O phishing_dataset.csv https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"phishing_dataset.csv\")\n",
        "\n",
        "# Display first 5 rows\n",
        "print(df.head())\n",
        "\n",
        "# Get dataset info\n",
        "df.info()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbf5VPMIPVtb",
        "outputId": "a90ab133-885c-48d4-acfb-d3f00cfe62e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   qty_dot_url  qty_hyphen_url  qty_underline_url  qty_slash_url  \\\n",
            "0            3               0                  0              1   \n",
            "1            5               0                  1              3   \n",
            "2            2               0                  0              1   \n",
            "3            4               0                  2              5   \n",
            "4            2               0                  0              0   \n",
            "\n",
            "   qty_questionmark_url  qty_equal_url  qty_at_url  qty_and_url  \\\n",
            "0                     0              0           0            0   \n",
            "1                     0              3           0            2   \n",
            "2                     0              0           0            0   \n",
            "3                     0              0           0            0   \n",
            "4                     0              0           0            0   \n",
            "\n",
            "   qty_exclamation_url  qty_space_url  ...  qty_ip_resolved  qty_nameservers  \\\n",
            "0                    0              0  ...                1                2   \n",
            "1                    0              0  ...                1                2   \n",
            "2                    0              0  ...                1                2   \n",
            "3                    0              0  ...                1                2   \n",
            "4                    0              0  ...                1                2   \n",
            "\n",
            "   qty_mx_servers  ttl_hostname  tls_ssl_certificate  qty_redirects  \\\n",
            "0               0           892                    0              0   \n",
            "1               1          9540                    1              0   \n",
            "2               3           589                    1              0   \n",
            "3               0           292                    1              0   \n",
            "4               1          3597                    0              1   \n",
            "\n",
            "   url_google_index  domain_google_index  url_shortened  phishing  \n",
            "0                 0                    0              0         1  \n",
            "1                 0                    0              0         1  \n",
            "2                 0                    0              0         0  \n",
            "3                 0                    0              0         1  \n",
            "4                 0                    0              0         0  \n",
            "\n",
            "[5 rows x 112 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 88647 entries, 0 to 88646\n",
            "Columns: 112 entries, qty_dot_url to phishing\n",
            "dtypes: float64(1), int64(111)\n",
            "memory usage: 75.7 MB\n",
            "\n",
            "Missing values:\n",
            " qty_dot_url             0\n",
            "qty_hyphen_url          0\n",
            "qty_underline_url       0\n",
            "qty_slash_url           0\n",
            "qty_questionmark_url    0\n",
            "                       ..\n",
            "qty_redirects           0\n",
            "url_google_index        0\n",
            "domain_google_index     0\n",
            "url_shortened           0\n",
            "phishing                0\n",
            "Length: 112, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=[\"phishing\",\"domain_in_ip\",\"time_response\",\"domain_spf\",\"asn_ip\",\"time_domain_activation\",\"time_domain_expiration\",\"qty_ip_resolved\",\"qty_nameservers\",\"qty_mx_servers\",\"ttl_hostname\",\"tls_ssl_certificate\",\"qty_redirects\",\"url_google_index\",\"domain_google_index\"]) #Features\n",
        "y = df[\"phishing\"] #Target variable"
      ],
      "metadata": {
        "id": "4I5KfVlYPiIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_95vE0WQRsTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqP9zm2YJf3N",
        "outputId": "0fe12ded-9ba0-4151-9be8-8a4b67d98a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: keras\n",
            "Version: 3.9.1\n",
            "Summary: Multi-backend Keras\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Keras team <keras-users@googlegroups.com>\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
            "Required-by: tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(8, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")  # Output: Probability of phishing\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U_BDBEtR7Sv",
        "outputId": "e900df50-3877-4518-ce59-4c506f1f1979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4433/4433 [==============================] - 16s 2ms/step - loss: 0.2397 - accuracy: 0.9025 - val_loss: 0.2156 - val_accuracy: 0.9107\n",
            "Epoch 2/10\n",
            "4433/4433 [==============================] - 10s 2ms/step - loss: 0.2161 - accuracy: 0.9102 - val_loss: 0.2060 - val_accuracy: 0.9127\n",
            "Epoch 3/10\n",
            "4433/4433 [==============================] - 10s 2ms/step - loss: 0.2100 - accuracy: 0.9116 - val_loss: 0.2004 - val_accuracy: 0.9155\n",
            "Epoch 4/10\n",
            "4433/4433 [==============================] - 10s 2ms/step - loss: 0.2076 - accuracy: 0.9113 - val_loss: 0.2032 - val_accuracy: 0.9119\n",
            "Epoch 5/10\n",
            "4433/4433 [==============================] - 10s 2ms/step - loss: 0.2053 - accuracy: 0.9129 - val_loss: 0.2070 - val_accuracy: 0.9136\n",
            "Epoch 6/10\n",
            "4433/4433 [==============================] - 10s 2ms/step - loss: 0.2038 - accuracy: 0.9130 - val_loss: 0.1985 - val_accuracy: 0.9156\n",
            "Epoch 7/10\n",
            "4433/4433 [==============================] - 11s 2ms/step - loss: 0.2026 - accuracy: 0.9146 - val_loss: 0.1962 - val_accuracy: 0.9135\n",
            "Epoch 8/10\n",
            "4433/4433 [==============================] - 10s 2ms/step - loss: 0.2011 - accuracy: 0.9145 - val_loss: 0.1960 - val_accuracy: 0.9156\n",
            "Epoch 9/10\n",
            "4433/4433 [==============================] - 11s 3ms/step - loss: 0.2004 - accuracy: 0.9143 - val_loss: 0.1939 - val_accuracy: 0.9172\n",
            "Epoch 10/10\n",
            "4433/4433 [==============================] - 12s 3ms/step - loss: 0.1999 - accuracy: 0.9146 - val_loss: 0.1918 - val_accuracy: 0.9179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7rKrOWeSHF8",
        "outputId": "e7246ce2-267e-4651-8f7e-71006c4dd0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "555/555 [==============================] - 1s 1ms/step - loss: 0.1918 - accuracy: 0.9179\n",
            "Test Accuracy: 91.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing with real urls"
      ],
      "metadata": {
        "id": "Scw3Ce5RS40r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tldextract whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LLWX98PSzmX",
        "outputId": "b34a56b6-15b9-4d86-adcc-75fc6e883788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (5.1.3)\n",
            "Requirement already satisfied: whois in /usr/local/lib/python3.11/dist-packages (1.20240129.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define a feature extration function"
      ],
      "metadata": {
        "id": "NiIZYukpTCp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "import socket\n",
        "import ssl\n",
        "import tldextract\n",
        "import whois\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to extract various URL features\n",
        "def extract_url_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # General URL features\n",
        "    features[\"qty_dot_url\"] = url.count('.')\n",
        "    features[\"qty_hyphen_url\"] = url.count('-')\n",
        "    features[\"qty_underline_url\"] = url.count('_')\n",
        "    features[\"qty_slash_url\"] = url.count('/')\n",
        "    features[\"qty_questionmark_url\"] = url.count('?')\n",
        "    features[\"qty_equal_url\"] = url.count('=')\n",
        "    features[\"qty_at_url\"] = url.count('@')\n",
        "    features[\"qty_and_url\"] = url.count('&')\n",
        "    features[\"qty_exclamation_url\"] = url.count('!')\n",
        "    features[\"qty_space_url\"] = url.count(' ')\n",
        "    features[\"qty_tilde_url\"] = url.count('~')\n",
        "    features[\"qty_comma_url\"] = url.count(',')\n",
        "    features[\"qty_plus_url\"] = url.count('+')\n",
        "    features[\"qty_asterisk_url\"] = url.count('*')\n",
        "    features[\"qty_hashtag_url\"] = url.count('#')\n",
        "    features[\"qty_dollar_url\"] = url.count('$')\n",
        "    features[\"qty_percent_url\"] = url.count('%')\n",
        "    features[\"qty_tld_url\t\"] = len(urlparse(url).hostname.split('.')[-1])\n",
        "    features[\"length_url\"] = len(url)\n",
        "\n",
        "    # Parse domain from URL\n",
        "    parsed_url = urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "\n",
        "    # Domain-related features\n",
        "    features[\"qty_dot_domain\"] = domain.count('.')\n",
        "    features[\"qty_hyphen_domain\"] = domain.count('-')\n",
        "    features[\"qty_underline_domain\"] = domain.count('_')\n",
        "    features[\"qty_slash_domain\"] = domain.count('/')\n",
        "    features[\"qty_questionmark_domain\"] = domain.count('?')\n",
        "    features[\"qty_equal_domain\"] = domain.count('=')\n",
        "    features[\"qty_at_domain\"] = domain.count('@')\n",
        "    features[\"qty_and_domain\"] = domain.count('&')\n",
        "    features[\"qty_exclamation_domain\"] = domain.count('!')\n",
        "    features[\"qty_space_domain\"] = domain.count(' ')\n",
        "    features[\"qty_tilde_domain\"] = domain.count('~')\n",
        "    features[\"qty_comma_domain\"] = domain.count(',')\n",
        "    features[\"qty_plus_domain\"] = domain.count('+')\n",
        "    features[\"qty_asterisk_domain\"] = domain.count('*')\n",
        "    features[\"qty_hashtag_domain\"] = domain.count('#')\n",
        "    features[\"qty_dollar_domain\"] = domain.count('$')\n",
        "    features[\"qty_percent_domain\"] = domain.count('%')\n",
        "    features[\"qty_vowels_domain\"] = sum(1 for c in domain if c in 'aeiou')\n",
        "    features[\"domain_length\"] = len(domain)\n",
        "\n",
        "    # # Resolve the domain to an IP\n",
        "    # try:\n",
        "    #     ip = socket.gethostbyname(domain)\n",
        "    #     features[\"domain_in_ip\"] = 1\n",
        "    # except socket.gaierror:\n",
        "    #     features[\"domain_in_ip\"] = 0\n",
        "\n",
        "    features[\"server_client_domain\"] = 1 if 'server' in domain or 'client' in domain else 0\n",
        "\n",
        "    directory = parsed_url.path\n",
        "\n",
        "    features[\"qty_dot_directory\"] = directory.count(\".\")\n",
        "    features[\"qty_hyphen_directory\"] = directory.count(\"-\")\n",
        "    features[\"qty_underline_directory\"] = directory.count(\"_\")\n",
        "    features[\"qty_slash_directory\"] = directory.count(\"/\")\n",
        "    features[\"qty_questionmark_directory\"] = directory.count(\"?\")\n",
        "    features[\"qty_equal_directory\"] = directory.count(\"=\")\n",
        "    features[\"qty_at_directory\"] = directory.count(\"@\")\n",
        "    features[\"qty_and_directory\"] = directory.count(\"&\")\n",
        "    features[\"qty_exclamation_directory\"] = directory.count(\"!\")\n",
        "    features[\"qty_space_directory\"] = directory.count(\" \")\n",
        "    features[\"qty_tilde_directory\"] = directory.count(\"~\")\n",
        "    features[\"qty_comma_directory\"] = directory.count(\",\")\n",
        "    features[\"qty_plus_directory\"] = directory.count(\"+\")\n",
        "    features[\"qty_asterisk_directory\"] = directory.count(\"*\")\n",
        "    features[\"qty_hashtag_directory\"] = directory.count(\"#\")\n",
        "    features[\"qty_dollar_directory\"] = directory.count(\"$\")\n",
        "    features[\"qty_percent_directory\"] = directory.count(\"%\")\n",
        "    features[\"directory_length\"] = len(directory)\n",
        "\n",
        "    file_part = parsed_url.path.split(\"/\")[-1]  # Last part after \"/\"\n",
        "\n",
        "    features[\"qty_dot_file\"] = file_part.count(\".\")\n",
        "    features[\"qty_hyphen_file\"] = file_part.count(\"-\")\n",
        "    features[\"qty_underline_file\"] = file_part.count(\"_\")\n",
        "    features[\"qty_slash_file\"] = file_part.count(\"/\")\n",
        "    features[\"qty_questionmark_file\"] = file_part.count(\"?\")\n",
        "    features[\"qty_equal_file\"] = file_part.count(\"=\")\n",
        "    features[\"qty_at_file\"] = file_part.count(\"@\")\n",
        "    features[\"qty_and_file\"] = file_part.count(\"&\")\n",
        "    features[\"qty_exclamation_file\"] = file_part.count(\"!\")\n",
        "    features[\"qty_space_file\"] = file_part.count(\" \")\n",
        "    features[\"qty_tilde_file\"] = file_part.count(\"~\")\n",
        "    features[\"qty_comma_file\"] = file_part.count(\",\")\n",
        "    features[\"qty_plus_file\"] = file_part.count(\"+\")\n",
        "    features[\"qty_asterisk_file\"] = file_part.count(\"*\")\n",
        "    features[\"qty_hashtag_file\"] = file_part.count(\"#\")\n",
        "    features[\"qty_dollar_file\"] = file_part.count(\"$\")\n",
        "    features[\"qty_percent_file\"] = file_part.count(\"%\")\n",
        "    features[\"file_length\"] = len(file_part)\n",
        "\n",
        "    params = parsed_url.query\n",
        "\n",
        "    features[\"qty_dot_params\"] = params.count(\".\")\n",
        "    features[\"qty_hyphen_params\"] = params.count(\"-\")\n",
        "    features[\"qty_underline_params\"] = params.count(\"_\")\n",
        "    features[\"qty_slash_params\"] = params.count(\"/\")\n",
        "    features[\"qty_questionmark_params\"] = params.count(\"?\")\n",
        "    features[\"qty_equal_params\"] = params.count(\"=\")\n",
        "    features[\"qty_at_params\"] = params.count(\"@\")\n",
        "    features[\"qty_and_params\"] = params.count(\"&\")\n",
        "    features[\"qty_exclamation_params\"] = params.count(\"!\")\n",
        "    features[\"qty_space_params\"] = params.count(\" \")\n",
        "    features[\"qty_tilde_params\"] = params.count(\"~\")\n",
        "    features[\"qty_comma_params\"] = params.count(\",\")\n",
        "    features[\"qty_plus_params\"] = params.count(\"+\")\n",
        "    features[\"qty_asterisk_params\"] = params.count(\"*\")\n",
        "    features[\"qty_hashtag_params\"] = params.count(\"#\")\n",
        "    features[\"qty_dollar_params\"] = params.count(\"$\")\n",
        "    features[\"qty_percent_params\"] = params.count(\"%\")\n",
        "    features[\"params_length\"] = len(params)\n",
        "\n",
        "    tlds = [\".com\", \".org\", \".net\", \".gov\", \".edu\"]  # Add more TLDs as needed\n",
        "    features[\"tld_present_params\"] = any(tld in params for tld in tlds)\n",
        "\n",
        "    features[\"qty_params\"] = len(params.split(\"&\")) if params else 0\n",
        "\n",
        "    features[\"email_in_url\"] = 1 if \"@\" in url else 0\n",
        "\n",
        "    # import time\n",
        "\n",
        "    # try:\n",
        "    #     start = time.time()\n",
        "    #     socket.gethostbyname(domain)\n",
        "    #     features[\"time_response\"] = time.time() - start\n",
        "    # except socket.gaierror:\n",
        "    #     features[\"time_response\"] = -1  # If resolution fails\n",
        "\n",
        "    # import dns.resolver\n",
        "\n",
        "    # try:\n",
        "    #     dns.resolver.resolve(domain, 'TXT')\n",
        "    #     features[\"domain_spf\"] = 1  # SPF record present\n",
        "    # except dns.resolver.NoAnswer:\n",
        "    #     features[\"domain_spf\"] = 0  # No SPF record\n",
        "    # except dns.resolver.NXDOMAIN:\n",
        "    #     features[\"domain_spf\"] = 0  # Domain does not exist\n",
        "\n",
        "    # import requests\n",
        "\n",
        "    # try:\n",
        "    #     ip_info = requests.get(f'https://ipinfo.io/{ip}/json').json()\n",
        "    #     features[\"asn_ip\"] = ip_info.get(\"org\", \"\").split()[-1]  # Extract ASN from the org field\n",
        "    # except requests.RequestException:\n",
        "    #     features[\"asn_ip\"] = None  # Handle failure\n",
        "\n",
        "    # # Using WHOIS to get domain information\n",
        "    # try:\n",
        "    #     domain_info = whois.whois(domain)\n",
        "    #     activation_date = domain_info.creation_date\n",
        "    #     expiration_date = domain_info.expiration_date\n",
        "    #     features[\"time_domain_activation\"] = (datetime.now() - activation_date).days if activation_date else -1\n",
        "    #     features[\"time_domain_expiration\"] = (expiration_date - datetime.now()).days if expiration_date else -1\n",
        "    # except Exception as e:\n",
        "    #     features[\"time_domain_activation\"] = -1\n",
        "    #     features[\"time_domain_expiration\"] = -1\n",
        "\n",
        "    # try:\n",
        "    #     ip_list = socket.gethostbyname_ex(domain)[2]\n",
        "    #     features[\"qty_ip_resolved\"] = len(ip_list)\n",
        "    # except socket.gaierror:\n",
        "    #     features[\"qty_ip_resolved\"] = 0\n",
        "\n",
        "    # import dns.resolver\n",
        "\n",
        "    # try:\n",
        "    #     nameservers = dns.resolver.resolve(domain, 'NS')\n",
        "    #     features[\"qty_nameservers\"] = len(nameservers)\n",
        "    # except dns.resolver.NoAnswer:\n",
        "    #     features[\"qty_nameservers\"] = 0\n",
        "\n",
        "    # try:\n",
        "    #     mx_records = dns.resolver.resolve(domain, 'MX')\n",
        "    #     features[\"qty_mx_servers\"] = len(mx_records)\n",
        "    # except dns.resolver.NoAnswer:\n",
        "    #     features[\"qty_mx_servers\"] = 0\n",
        "\n",
        "    # try:\n",
        "    #     ttl = dns.resolver.resolve(domain)[0].ttl\n",
        "    #     features[\"ttl_hostname\"] = ttl\n",
        "    # except dns.resolver.NoAnswer:\n",
        "    #     features[\"ttl_hostname\"] = None\n",
        "\n",
        "    # # Check SSL certificate validity\n",
        "    # try:\n",
        "    #     conn = ssl.create_default_context().wrap_socket(socket.socket(), server_hostname=domain)\n",
        "    #     conn.connect((domain, 443))\n",
        "    #     features[\"tls_ssl_certificate\"] = 1\n",
        "    # except Exception:\n",
        "    #     features[\"tls_ssl_certificate\"] = 0\n",
        "\n",
        "    # # Count number of redirects (by following the URL)\n",
        "    # try:\n",
        "    #     response = requests.get(url, allow_redirects=True)\n",
        "    #     features[\"qty_redirects\"] = len(response.history)\n",
        "    # except requests.RequestException:\n",
        "    #     features[\"qty_redirects\"] = 0\n",
        "\n",
        "    # # Google index check (simplified check using requests)\n",
        "    # try:\n",
        "    #     google_search_url = f\"https://www.google.com/search?q=site:{domain}\"\n",
        "    #     response = requests.get(google_search_url)\n",
        "    #     features[\"url_google_index\"] = 'No results' not in response.text\n",
        "    # except requests.RequestException:\n",
        "    #     features[\"url_google_index\"] = 0\n",
        "\n",
        "    # try:\n",
        "    #     google_search_url = f\"https://www.google.com/search?q=site:{domain}\"\n",
        "    #     response = requests.get(google_search_url)\n",
        "    #     features[\"domain_google_index\"] = 'No results' not in response.text\n",
        "    # except requests.RequestException:\n",
        "    #     features[\"domain_google_index\"] = 0\n",
        "\n",
        "    shortened_domains = [\"bit.ly\", \"goo.gl\", \"t.co\", \"tinyurl.com\"]  # Add more shortened domains\n",
        "    features[\"url_shortened\"] = 1 if any(shortened_domain in url for shortened_domain in shortened_domains) else 0\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "1dyrZZmkTBDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dnspython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc2iCJpwfer7",
        "outputId": "f8d83a8d-c266-4874-efa1-4c53d1ca8c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.11/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_url(url):\n",
        "    # Extract features from the URL\n",
        "    features = extract_url_features(url)\n",
        "\n",
        "    # Prepare the features as a list (for a single prediction)\n",
        "    feature_vector = np.array([list(features.values())], dtype=np.float32)\n",
        "\n",
        "    # Use the trained model to make a prediction\n",
        "    prediction = model.predict(feature_vector)\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "url_to_predict = \"http://example.com\"\n",
        "prediction = predict_url(url_to_predict)\n",
        "\n",
        "print(f\"The prediction for the URL {url_to_predict} is: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb7cZd8eeV2n",
        "outputId": "acf2bbf8-4ec6-4f9c-82fe-d0c27479baae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
            "The prediction for the URL http://example.com is: [[0.8109498]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_to_predict = \"http://sharedpont77wwjxjx.df.r.appspot.com/wp-content/xp.php\"\n",
        "prediction = predict_url(url_to_predict)\n",
        "\n",
        "print(f\"The prediction for the URL {url_to_predict} is: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-EN2XOG03Ef",
        "outputId": "3b56693b-2773-42c9-f896-b8d77c60b560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "The prediction for the URL http://sharedpont77wwjxjx.df.r.appspot.com/wp-content/xp.php is: [[0.9999291]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRUonabb7zPu",
        "outputId": "b9662da7-23f6-472a-d0af-3a80108774f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                1568      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1713 (6.69 KB)\n",
            "Trainable params: 1713 (6.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w_KGynSTAjwM",
        "outputId": "088b8eab-9748-4531-b06f-c0fe45fcffe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.11/dist-packages (4.22.0)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.10.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.5.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.5.1)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (1.26.4)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.10)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.72)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.9)\n",
            "Collecting ml_dtypes>=0.4.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Using cached ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Using cached keras-3.9.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.11.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.1.3)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.89)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (1.12.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.12.1)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.21.0)\n",
            "Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "Using cached keras-3.9.1-py3-none-any.whl (1.3 MB)\n",
            "Using cached ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Installing collected packages: protobuf, ml_dtypes, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.9.1 ml_dtypes-0.5.1 protobuf-5.29.4 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "38cb89007a444f14a36ecb05183f221e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow_decision_forests==1.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CLxox0UULtHB",
        "outputId": "e23782dc-416f-4fbd-c64c-6711cfb2e1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_decision_forests==1.8.1\n",
            "  Downloading tensorflow_decision_forests-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (2.2.2)\n",
            "Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (2.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (1.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests==1.8.1) (3.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow_decision_forests==1.8.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow_decision_forests==1.8.1) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow_decision_forests==1.8.1) (2025.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests==1.8.1) (3.2.2)\n",
            "Downloading tensorflow_decision_forests-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow_decision_forests\n",
            "  Attempting uninstall: tensorflow_decision_forests\n",
            "    Found existing installation: tensorflow_decision_forests 1.12.0\n",
            "    Uninstalling tensorflow_decision_forests-1.12.0:\n",
            "      Successfully uninstalled tensorflow_decision_forests-1.12.0\n",
            "Successfully installed tensorflow_decision_forests-1.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yggdrasil_decision_forests"
                ]
              },
              "id": "29e37726dbea4fc9b346a07fca655150"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflowjs as tfjs\n",
        "\n",
        "tfjs.converters.save_keras_model(model, 'tfjs_model')"
      ],
      "metadata": {
        "id": "Si8IZWInB43K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab45209-288d-43b5-8609-6bb67a447691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('tfjs_model', 'zip', 'tfjs_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vV-2DQmgE7mk",
        "outputId": "4ff55a49-125c-44b7-da45-4a39e9df84ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tfjs_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('tfjs_model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "e0mvZ0ziE9V3",
        "outputId": "1d0f2e34-8ab1-4194-d2fd-983a80f147da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ad4716f9-2c96-48b0-913b-5172cfbe5ff2\", \"tfjs_model.zip\", 7402)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}